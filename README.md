# MEASURING ROBUSTNESS OF IMAGE CLASSIFICATION MODELS 

I use a variant of the Fast Gradient Signed Method to alter images so my own trained ML models misclassify them. I work with the CIFAR-10 Dataset and the MNIST and eplore how hyper paramaters and model classes affect how vulnerable a model is to attack. 

Check out the [full paper](https://github.com/vs1720/Neural_Attack/blob/main/Attack_NN_paper.pdf)
